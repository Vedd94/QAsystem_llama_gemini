{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c38f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5255afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ebde56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api key found\n"
     ]
    }
   ],
   "source": [
    "if google_api_key=='':\n",
    "    print('api key not found')\n",
    "else:\n",
    "    print('api key found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c1b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "import google.generativeai as genai\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe62d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84477af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-vision-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
      "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
      "                   'Move to a newer Gemini version.'),\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-pro-vision',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
      "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
      "                   'Move to a newer Gemini version.'),\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-1.5-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
      "                   'million tokens.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Pro 002',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
      "                   'across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
      "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Flash 002',\n",
      "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B 001',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
      "                   'released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.5-pro-exp-03-25',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini 2.5 Pro Experimental 03-25',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro-preview-03-25',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-03-25',\n",
      "      display_name='Gemini 2.5 Pro Preview 03-25',\n",
      "      description='Gemini 2.5 Pro Preview 03-25',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-preview-04-17',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-04-17',\n",
      "      display_name='Gemini 2.5 Flash Preview 04-17',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-preview-05-20',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.5 Flash',\n",
      "      description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
      "                   'supports up to 1 million tokens, released in June of 2025.'),\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-preview-04-17-thinking',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-04-17',\n",
      "      display_name='Gemini 2.5 Flash Preview 04-17 for cursor testing',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-lite-preview-06-17',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-06-17',\n",
      "      display_name='Gemini 2.5 Flash-Lite Preview 06-17',\n",
      "      description='Preview release (June 11th, 2025) of Gemini 2.5 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro-preview-05-06',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-06',\n",
      "      display_name='Gemini 2.5 Pro Preview 05-06',\n",
      "      description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro-preview-06-05',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-06-05',\n",
      "      display_name='Gemini 2.5 Pro Preview',\n",
      "      description='Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro',\n",
      "      base_model_id='',\n",
      "      version='2.5',\n",
      "      display_name='Gemini 2.5 Pro',\n",
      "      description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-exp',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash Experimental',\n",
      "      description='Gemini 2.0 Flash Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash',\n",
      "      description='Gemini 2.0 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash 001',\n",
      "      description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in January of 2025.'),\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
      "      description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-lite-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash-Lite 001',\n",
      "      description='Stable version of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-lite',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash-Lite',\n",
      "      description='Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-preview-image-generation',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash Preview Image Generation',\n",
      "      description='Gemini 2.0 Flash Preview Image Generation',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
      "      base_model_id='',\n",
      "      version='preview-02-05',\n",
      "      display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
      "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-lite-preview',\n",
      "      base_model_id='',\n",
      "      version='preview-02-05',\n",
      "      display_name='Gemini 2.0 Flash-Lite Preview',\n",
      "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-pro-exp',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini 2.0 Pro Experimental',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-pro-exp-02-05',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini 2.0 Pro Experimental 02-05',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-exp-1206',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-04-17',\n",
      "      display_name='Gemini 2.5 Flash Preview 04-17',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-04-17',\n",
      "      display_name='Gemini 2.5 Flash Preview 04-17',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-04-17',\n",
      "      display_name='Gemini 2.5 Flash Preview 04-17',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-preview-tts',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Preview TTS',\n",
      "      description='Gemini 2.5 Flash Preview TTS',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=16384,\n",
      "      supported_generation_methods=['countTokens', 'generateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro-preview-tts',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
      "      display_name='Gemini 2.5 Pro Preview TTS',\n",
      "      description='Gemini 2.5 Pro Preview TTS',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=16384,\n",
      "      supported_generation_methods=['countTokens', 'generateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/learnlm-2.0-flash-experimental',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='LearnLM 2.0 Flash Experimental',\n",
      "      description='LearnLM 2.0 Flash Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=32768,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-1b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 1B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-4b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 4B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-12b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 12B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-27b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 27B',\n",
      "      description='',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3n-e4b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3n E4B',\n",
      "      description='',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/text-embedding-004',\n",
      "      base_model_id='',\n",
      "      version='004',\n",
      "      display_name='Text Embedding 004',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-embedding-exp-03-07',\n",
      "      base_model_id='',\n",
      "      version='exp-03-07',\n",
      "      display_name='Gemini Embedding Experimental 03-07',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-embedding-exp',\n",
      "      base_model_id='',\n",
      "      version='exp-03-07',\n",
      "      display_name='Gemini Embedding Experimental',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n",
      "Model(name='models/imagen-3.0-generate-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Imagen 3.0 002 model',\n",
      "      description='Vertex served Imagen 3.0 002 model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-generate-preview-06-06',\n",
      "      base_model_id='',\n",
      "      version='01',\n",
      "      display_name='Imagen 4 (Preview)',\n",
      "      description='Vertex served Imagen 4.0 model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
      "      base_model_id='',\n",
      "      version='01',\n",
      "      display_name='Imagen 4 Ultra (Preview)',\n",
      "      description='Vertex served Imagen 4.0 ultra model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/veo-2.0-generate-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Veo 2',\n",
      "      description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
      "                   'enabled on the associated Google Cloud Platform account. Please visit '\n",
      "                   'https://console.cloud.google.com/billing to enable it.'),\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-2.5-flash-preview-native-audio-dialog',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
      "      description='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-exp-native-audio-thinking-dialog',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-exp-native-audio-thinking-dialog-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
      "      description='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-live-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.0 Flash 001',\n",
      "      description='Gemini 2.0 Flash 001',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-live-2.5-flash-preview',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Live 2.5 Flash Preview',\n",
      "      description='Gemini Live 2.5 Flash Preview',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n"
     ]
    }
   ],
   "source": [
    "for models in genai.list_models():\n",
    "    print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea04458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-04-17\n",
      "models/gemini-2.5-flash-preview-05-20\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-flash-preview-04-17-thinking\n",
      "models/gemini-2.5-flash-lite-preview-06-17\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-preview-image-generation\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n"
     ]
    }
   ],
   "source": [
    "for models in genai.list_models():\n",
    "    if'generateContent' in models.supported_generation_methods:\n",
    "        print(models.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae00871a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SimpleDirectoryReader.load_data of <llama_index.core.readers.file.base.SimpleDirectoryReader object at 0x000001D818340C20>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('../Data')\n",
    "documents.load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ffd0a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = documents.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4241dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub.\n",
      "User name - Vedd94\n",
      "Password - 143mylife@123\n",
      "\n",
      "\n",
      "Steam\n",
      "user name - ii__ved__ii\n",
      "Password - Asv@12345\n",
      "\n",
      "HackerRAnk\n",
      "password - Asv@12345\n",
      "\n",
      "Valorent\n",
      "username - WinterMaguire007\n",
      "password - 143mylife@123\n",
      "\n",
      "TPIN - 170556\n"
     ]
    }
   ],
   "source": [
    "print(doc[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c20f0ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ved Dahale\\AppData\\Local\\Temp\\ipykernel_18564\\3909338345.py:1: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  model = Gemini(models='gemini-pro', api_key=google_api_key)\n",
      "C:\\Users\\Ved Dahale\\AppData\\Local\\Temp\\ipykernel_18564\\3909338345.py:2: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
      "  gemini_embed_model=GeminiEmbedding(model_name = 'models/embedding-001')\n"
     ]
    }
   ],
   "source": [
    "model = Gemini(models='gemini-pro', api_key=google_api_key)\n",
    "gemini_embed_model=GeminiEmbedding(model_name = 'models/embedding-001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6702dfe6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ServiceContext is deprecated. Use llama_index.settings.Settings instead, or pass in modules to local functions/methods/interfaces.\nSee the docs for updated usage/migration: \nhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m service_context = \u001b[43mServiceContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgemini_embed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAI\\QAsystem_llama_gemini\\venv\\Lib\\site-packages\\llama_index\\core\\service_context.py:33\u001b[39m, in \u001b[36mServiceContext.from_defaults\u001b[39m\u001b[34m(cls, **kwargs)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_defaults\u001b[39m(\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m     24\u001b[39m     **kwargs: Any,\n\u001b[32m     25\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mServiceContext\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    Create a ServiceContext from defaults.\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m \n\u001b[32m     32\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     34\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mServiceContext is deprecated. Use llama_index.settings.Settings instead, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor pass in modules to local functions/methods/interfaces.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSee the docs for updated usage/migration: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     38\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: ServiceContext is deprecated. Use llama_index.settings.Settings instead, or pass in modules to local functions/methods/interfaces.\nSee the docs for updated usage/migration: \nhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=model, embed_model = gemini_embed_model,chunk_size = 400, overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de1145f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "# Set these globally\n",
    "Settings.llm = model\n",
    "Settings.embed_model = gemini_embed_model\n",
    "Settings.chunk_size = 400\n",
    "Settings.chunk_overlap = 20\n",
    "\n",
    "service_context = [Settings.llm, Settings.embed_model, Settings.chunk_size, Settings.chunk_overlap]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edce6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(doc, service_context = service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bb5d7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x1d81b6e81a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0572c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0836617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe73bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query('What is my valorant account password?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0caf1937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The password for the Valorant account is 143mylife@123.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56e9b7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='The provided text does not contain information about MS Dhoni.\\n', source_nodes=[NodeWithScore(node=TextNode(id_='3bd78548-cd48-4c6d-90db-e38416ef7582', embedding=None, metadata={'file_path': 'd:\\\\GenAI\\\\QAsystem_llama_gemini\\\\notebook\\\\..\\\\Data\\\\Imp.txt', 'file_name': 'Imp.txt', 'file_type': 'text/plain', 'file_size': 231, 'creation_date': '2025-06-25', 'last_modified_date': '2022-02-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ea3f495c-1c1a-4ff0-8785-67b9ac8c9cef', node_type='4', metadata={'file_path': 'd:\\\\GenAI\\\\QAsystem_llama_gemini\\\\notebook\\\\..\\\\Data\\\\Imp.txt', 'file_name': 'Imp.txt', 'file_type': 'text/plain', 'file_size': 231, 'creation_date': '2025-06-25', 'last_modified_date': '2022-02-14'}, hash='3bc38805e61f80f65e151b46ee1332a782edc307b51183a0fc4ca07978987c98')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='GitHub.\\r\\nUser name - Vedd94\\r\\nPassword - 143mylife@123\\r\\n\\r\\n\\r\\nSteam\\r\\nuser name - ii__ved__ii\\r\\nPassword - Asv@12345\\r\\n\\r\\nHackerRAnk\\r\\npassword - Asv@12345\\r\\n\\r\\nValorent\\r\\nusername - WinterMaguire007\\r\\npassword - 143mylife@123\\r\\n\\r\\nTPIN - 170556', mimetype='text/plain', start_char_idx=0, end_char_idx=231, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.641863827676846)], metadata={'3bd78548-cd48-4c6d-90db-e38416ef7582': {'file_path': 'd:\\\\GenAI\\\\QAsystem_llama_gemini\\\\notebook\\\\..\\\\Data\\\\Imp.txt', 'file_name': 'Imp.txt', 'file_type': 'text/plain', 'file_size': 231, 'creation_date': '2025-06-25', 'last_modified_date': '2022-02-14'}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query('Who is MS Dhoni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feca341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
